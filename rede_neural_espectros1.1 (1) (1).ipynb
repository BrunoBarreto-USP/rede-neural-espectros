{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cf9aa4",
   "metadata": {},
   "source": [
    "## Rede Neural para Classificação de espectros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# imports do astropy\n",
    "from astropy.io import fits\n",
    "\n",
    "# imports do sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "# imports do tensorflow e keras\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import  Input, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import traceback\n",
    "# para liberar memoria\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57596469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Parâmetros & Configuração ---\n",
    "CONFIG = {\n",
    "    \"fits_directory\": Path('D:\\sdss_spectra'),\n",
    "    \"target_length\": 4000,\n",
    "    \"test_set_size\": 0.10,\n",
    "    \"n_splits\": 3,\n",
    "    \"shuffle_kfold\": True,\n",
    "    \"random_state_kfold\": 37,\n",
    "    \"random_state_test_split\": 132,\n",
    "    \"epochs\": 150,\n",
    "    \"final_model_epochs\": 150,\n",
    "    \"batch_size\": 128,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"redshift_filter_threshold\": 1e-9,\n",
    "    \"pool_size\": 4,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"initial_learning_rate\": 0.001,\n",
    "    \"decay_steps\": 5000,\n",
    "    \"alpha\": 0,\n",
    "    \"N_COMPONENTS\": 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from tqdm.keras import TqdmCallback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open specObj-dr17.fits\n",
    "specObj = fits.open('specObj-dr17.fits')\n",
    "#specObj.info() # to see the structure of the file\n",
    "#specObj[1].header # to see the header of the first extension\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8695cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(specobj_fits_file, fits_files: list[Path]) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Filtra arquivos FITS de acordo com critérios de qualidade no catálogo specObj.\n",
    "    Retorna apenas aqueles com PLATEQUALITY='good', ZWARNING=0 e SPECPRIMARY=1.\n",
    "    Mostra uma barra de progresso do processamento.\n",
    "    \"\"\"\n",
    "    # 1) Carrega catálogo specObj\n",
    "    with fits.open(specobj_fits_file) as hdul:\n",
    "        data = hdul[1].data\n",
    "        \n",
    "        # converte cada coluna em série pandas para permitir .str\n",
    "        platequality = pd.Series(\n",
    "            data['PLATEQUALITY']\n",
    "            .byteswap().newbyteorder()\n",
    "            .astype(str)\n",
    "        ).str.strip().str.lower()\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'PLATE':         data['PLATE'].byteswap().newbyteorder(),\n",
    "            'MJD':           data['MJD'].byteswap().newbyteorder(),\n",
    "            'FIBERID':       data['FIBERID'].byteswap().newbyteorder(),\n",
    "            'PLATEQUALITY':  platequality,\n",
    "            'ZWARNING':      data['ZWARNING'].byteswap().newbyteorder(),\n",
    "            'SPECPRIMARY':   data['SPECPRIMARY'].byteswap().newbyteorder(),\n",
    "        })\n",
    "\n",
    "    # 2) Aplica filtros\n",
    "    mask = (\n",
    "        (df['PLATEQUALITY'] == 'good') &\n",
    "        (df['ZWARNING']    == 0)    &\n",
    "        (df['SPECPRIMARY'] == 1)\n",
    "    )\n",
    "    df_filtered = df.loc[mask, ['PLATE', 'MJD', 'FIBERID']]\n",
    "\n",
    "    valid_keys = set(zip(\n",
    "        df_filtered['PLATE'],\n",
    "        df_filtered['MJD'],\n",
    "        df_filtered['FIBERID']\n",
    "    ))\n",
    "\n",
    "    # 3) Filtra lista de arquivos pelo nome, mostrando progresso\n",
    "    valid_files = []\n",
    "    for f in tqdm(fits_files, desc=\"Limpando arquivos FITS\", unit=\"arquivo\"):\n",
    "        parts = f.stem.split('-')\n",
    "        # esperamos: ['spec', plate, mjd, fiber, class, subclass, redshift]\n",
    "        if len(parts) >= 7:\n",
    "            try:\n",
    "                key = (int(parts[1]), int(parts[2]), int(parts[3]))\n",
    "                if key in valid_keys:\n",
    "                    valid_files.append(f)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return valid_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1827a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess(directory: Path, target_length: int):\n",
    "    \"\"\"\n",
    "    Carrega espectros FITS de um diretório, extrai fluxo, redshift e classe.\n",
    "    Se o header não contiver CLASS, faz fallback à classe e redshift\n",
    "    codificados no nome do arquivo: spec-plate-mjd-fiber-class-subclass-redshift.fits\n",
    "    Paralelizado para acelerar o carregamento.\n",
    "    \"\"\"\n",
    "    files = list(Path(directory).glob('spec-*.fits'))\n",
    "    print(f\"→ Encontrados {len(files)} arquivos FITS no diretório.\")\n",
    "    print(f\"→ Filtrando arquivos para garantir qualidade...\")\n",
    "    files = data_cleaning('specObj-dr17.fits', files)\n",
    "    print(f\"→ Encontrados {len(files)} arquivos após o filtro.\")\n",
    "\n",
    "    flux_list, redshifts, ivar_list, labels = [], [], [], []\n",
    "    skipped = 0\n",
    "\n",
    "    def process_file(f):\n",
    "        try:\n",
    "            with fits.open(f, memmap=False) as hdul:\n",
    "                hdr = hdul[0].header\n",
    "                classe = hdr.get('CLASS', '').strip().upper()\n",
    "                red = hdr.get('Z', np.nan)\n",
    "                if not classe or np.isnan(red):\n",
    "                    parts = f.stem.split('-')\n",
    "                    if len(parts) >= 7:\n",
    "                        classe = parts[4].upper()\n",
    "                        try:\n",
    "                            red = float(parts[-1])\n",
    "                        except ValueError:\n",
    "                            red = np.nan\n",
    "                if not classe or np.isnan(red):\n",
    "                    return None\n",
    "                flux = hdul[1].data['flux'].astype(np.float32)\n",
    "                ivar = hdul[1].data['ivar'].astype(np.float32)\n",
    "                if flux.size != target_length:\n",
    "                    flux = np.pad(flux, (0, max(0, target_length - flux.size)), 'constant')[:target_length]\n",
    "                    ivar = np.pad(ivar, (0, max(0, target_length - ivar.size)), 'constant')[:target_length]\n",
    "                return (flux, red, ivar, classe)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_file, f) for f in files]\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Processando espectros\", unit=\"arquivo\"):\n",
    "            result = fut.result()\n",
    "            if result is not None:\n",
    "                flux, red, ivar, classe = result\n",
    "                flux_list.append(flux)\n",
    "                redshifts.append(red)\n",
    "                ivar_list.append(ivar)\n",
    "                labels.append(classe)\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "    print(f\"→ Carregados: {len(flux_list)} espectros; Pulados: {skipped} arquivos.\")\n",
    "\n",
    "    if not flux_list:\n",
    "        raise ValueError(\"Nenhum espectro válido carregado.\")\n",
    "\n",
    "    X_flux = np.stack(flux_list)\n",
    "    X_z = np.array(redshifts, dtype=np.float32).reshape(-1, 1)\n",
    "    X_ivar = np.stack(ivar_list)\n",
    "    encoder = LabelEncoder().fit(labels)\n",
    "    y = encoder.transform(labels)\n",
    "\n",
    "    return X_flux, X_z, X_ivar, y, encoder.classes_, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caadf78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model_tuner_factory(n_flux_features, n_z_features, n_ivar_features, n_classes, config):\n",
    "    def build_model_tuner(hp):\n",
    "        input_spec = Input(shape=(n_flux_features,), name='Spectrum_Input')\n",
    "        input_z = Input(shape=(n_z_features,), name='Redshift_Input')\n",
    "        input_ivar = Input(shape=(n_ivar_features,), name='Ivar_Input')\n",
    "        x = concatenate([input_spec, input_z, input_ivar])\n",
    "\n",
    "        # Tune do numero de camadas e unidades\n",
    "        num_layers = hp.Int('num_layers', 2, 10)\n",
    "        for i in range(num_layers):\n",
    "            units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32)\n",
    "            x = Dense(units, activation='relu')(x)\n",
    "            if hp.Boolean(f'dropout_{i}'):\n",
    "                x = Dropout(rate=hp.Float(f'dropout_rate_{i}', 0.1, 0.5, step=0.1))(x)\n",
    "\n",
    "        output = Dense(n_classes, activation='softmax')(x)\n",
    "        # Define o learning rate schedule\n",
    "        initial_lr  = hp.Float('cosine_initial_lr',\n",
    "                                   1e-5, 1e-2, sampling='log')\n",
    "        decay_steps = hp.Int('cosine_decay_steps',\n",
    "                                 500, 5000, step=500)\n",
    "        alpha       = hp.Float('cosine_alpha',\n",
    "                                   0.0, 0.5, step=0.1)\n",
    "        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=initial_lr,\n",
    "                decay_steps=decay_steps,\n",
    "                alpha=alpha\n",
    "            )\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        model = Model(inputs=[input_spec, input_z, input_ivar], outputs=output)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    return build_model_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_scale(Xf, Xz, Xivar, y, config):\n",
    "    \"\"\"Split treino/K-fold pool vs. teste e retorna todos os conjuntos já escalados.\"\"\"\n",
    "    # Split treino/K-fold pool vs. teste final\n",
    "    Xf_k, Xf_test, Xz_k, Xz_test, Xivar_k, Xivar_test, y_k, y_test = train_test_split(\n",
    "        Xf, Xz, Xivar, y,\n",
    "        test_size=config[\"test_set_size\"],\n",
    "        random_state=config[\"random_state_test_split\"],\n",
    "        stratify=np.argmax(y, axis=1)\n",
    "    )\n",
    "    # Ajusta scalers no pool inteiro\n",
    "    flux_scaler = StandardScaler().fit(Xf_k)\n",
    "    z_scaler    = StandardScaler().fit(Xz_k)\n",
    "    ivar_scaler = StandardScaler().fit(Xivar_k)\n",
    "    # Transforma pool e teste\n",
    "    return (\n",
    "        flux_scaler.transform(Xf_k),\n",
    "        z_scaler.transform(Xz_k),\n",
    "        ivar_scaler.transform(Xivar_k),\n",
    "        y_k,\n",
    "        flux_scaler.transform(Xf_test),\n",
    "        z_scaler.transform(Xz_test),\n",
    "        ivar_scaler.transform(Xivar_test),\n",
    "        y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde018c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(Xf_k, Xz_k, Xivar_k, y_k, config,best__hp):\n",
    "    \"\"\"Executa StratifiedKFold, retorna listas de (loss, acc) e histories.\"\"\"\n",
    "    kf = StratifiedKFold(\n",
    "        n_splits=config[\"n_splits\"],\n",
    "        shuffle=config[\"shuffle_kfold\"],\n",
    "        random_state=config[\"random_state_kfold\"]\n",
    "    )\n",
    "    metrics = []\n",
    "    histories = []\n",
    "\n",
    "    for fold, (tr, va) in enumerate(kf.split(Xf_k, np.argmax(y_k, axis=1)), start=1):\n",
    "        print(f\"\\n--- Fold {fold}/{config['n_splits']} ---\")\n",
    "        Xf_tr, Xf_va = Xf_k[tr], Xf_k[va]\n",
    "        Xz_tr, Xz_va = Xz_k[tr], Xz_k[va]\n",
    "        Xivar_tr, Xivar_va = Xivar_k[tr], Xivar_k[va]\n",
    "        y_tr,  y_va  = y_k[tr],  y_k[va]\n",
    "\n",
    "        # Escala dentro do fold\n",
    "        fs = StandardScaler().fit(Xf_tr);    Xf_tr_s = fs.transform(Xf_tr);    Xf_va_s = fs.transform(Xf_va)\n",
    "        zs = StandardScaler().fit(Xz_tr);    Xz_tr_s = zs.transform(Xz_tr);    Xz_va_s = zs.transform(Xz_va)\n",
    "        ivars = StandardScaler().fit(Xivar_tr);    Xivar_tr_s = ivars.transform(Xivar_tr);    Xivar_va_s = ivars.transform(Xivar_va)\n",
    "\n",
    "        # Constrói e treina\n",
    "        model_builder = build_model_tuner_factory(\n",
    "            Xf_tr_s.shape[1],\n",
    "            Xz_tr_s.shape[1],\n",
    "            Xivar_tr_s.shape[1],\n",
    "            y_k.shape[1],\n",
    "            config\n",
    "        )\n",
    "        model = model_builder(best__hp)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=config[\"early_stopping_patience\"], restore_best_weights=True)\n",
    "        \n",
    "        hist = model.fit(\n",
    "            [Xf_tr_s, Xz_tr_s, Xivar_tr_s], y_tr,\n",
    "            validation_data=([Xf_va_s, Xz_va_s, Xivar_va_s], y_va),\n",
    "            epochs=config[\"epochs\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            callbacks=[es, TqdmCallback(verbose=0)],\n",
    "            verbose=0\n",
    "        )\n",
    "        histories.append(hist)\n",
    "\n",
    "        loss, acc = model.evaluate([Xf_va_s, Xz_va_s, Xivar_va_s], y_va, verbose=0)\n",
    "        print(f\"Fold {fold} → loss={loss:.4f}, acc={acc:.4f}\")\n",
    "        metrics.append((loss, acc))\n",
    "\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        del model\n",
    "\n",
    "    return metrics, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories(histories):\n",
    "    \"\"\"Plota curvas de loss e accuracy para cada fold.\"\"\"\n",
    "    for i, h in enumerate(histories, start=1):\n",
    "        epochs = range(1, len(h.history['loss']) + 1)\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for j, key in enumerate(['accuracy', 'loss'], start=1):\n",
    "            plt.subplot(1, 2, j)\n",
    "            plt.plot(epochs, h.history[key], label='Treino')\n",
    "            plt.plot(epochs, h.history[f'val_{key}'], label='Validação')\n",
    "            plt.title(f'Fold {i} — {key.capitalize()}')\n",
    "            plt.xlabel('Época')\n",
    "            plt.ylabel(key.capitalize())\n",
    "            plt.legend()\n",
    "            plt.grid('--', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068dcf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_and_evaluate(Xf_k_s, Xz_k_s, Xivar_k_s, y_k, Xf_test_s, Xz_test_s, Xivar_test_s, y_test, label_encoder, config, best_hp):\n",
    "    \"\"\"Treina no pool completo e avalia no teste final, exibindo matriz de confusão e relatório.\"\"\"\n",
    "    print(\"\\n--- Treinamento Final no Pool Completo ---\")\n",
    "    model_builder = build_model_tuner_factory(Xf_k_s.shape[1], Xz_k_s.shape[1], Xivar_k_s.shape[1] ,y_k.shape[1], config)\n",
    "    model = model_builder(best_hp)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=config[\"early_stopping_patience\"], restore_best_weights=True)\n",
    "    model.fit(\n",
    "        [Xf_k_s, Xz_k_s,Xivar_k_s], y_k,\n",
    "        validation_split=0.1,\n",
    "        epochs=config[\"final_model_epochs\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        callbacks=[es],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Avaliação no Conjunto de Teste ---\")\n",
    "    loss, acc = model.evaluate([Xf_test_s, Xz_test_s, Xivar_test_s], y_test, verbose=0)\n",
    "    print(f\"Perda teste: {loss:.4f} | Acurácia teste: {acc:.4f}\")\n",
    "\n",
    "    # Confusion matrix & report\n",
    "    y_pred = np.argmax(model.predict([Xf_test_s, Xz_test_s, Xivar_test_s]), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    lbls   = label_encoder.inverse_transform\n",
    "\n",
    "    cm = confusion_matrix(lbls(y_true), lbls(y_pred), labels=sorted(label_encoder.classes_))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.xlabel('Previsto'); plt.ylabel('Real'); plt.title('Matriz de Confusão')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(lbls(y_true), lbls(y_pred), zero_division=0))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # --- 4. Carrega & Pré-processa ---\n",
    "    # Note: load_and_preprocess já retorna 'y' como inteiros codificados\n",
    "    Xf, Xz_raw, Xivar_raw, y_int, classes, le = load_and_preprocess(\n",
    "        CONFIG[\"fits_directory\"],\n",
    "        CONFIG[\"target_length\"]\n",
    "    )\n",
    "    \n",
    "    #salvar Xf, Xz_raw, Xivar_raw, y_int, classes, le\n",
    "    np.savez('spectra_data.npz', Xf=Xf, Xz_raw=Xz_raw, Xivar_raw=Xivar_raw, y_int=y_int, classes=classes, le=le.classes_)\n",
    "\n",
    "    # --- 5. Filtrar pelo Redshift ---\n",
    "    mask = Xz_raw.flatten() > CONFIG[\"redshift_filter_threshold\"]\n",
    "    Xf, Xz_raw, Xivar_raw, y_int = Xf[mask], Xz_raw[mask], Xivar_raw[mask], y_int[mask]\n",
    "    print(f\"Após filtro z > {CONFIG['redshift_filter_threshold']}: {len(Xf)} espectros.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Erro durante o carregamento e pré-processamento:\", e)\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5003ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # descarrgear os dados salvos\n",
    "    data = np.load('spectra_data.npz')\n",
    "    Xf = data['Xf']\n",
    "    Xz_raw = data['Xz_raw']\n",
    "    Xivar_raw = data['Xivar_raw']\n",
    "    y_int = data['y_int']\n",
    "    classes = data['classes']\n",
    "    le = data['le']\n",
    "    print(f\"Dados carregados: {Xf.shape}, {Xz_raw.shape}, {Xivar_raw.shape}, {y_int.shape}, {classes}, {le}\")\n",
    "    # --- 6. One-hot encoding direto ---\n",
    "    y_oh = tf.keras.utils.to_categorical(y_int, num_classes=len(classes))\n",
    "    print(f\"Aplicando PCA para reduzir de {Xf.shape[1]} para {CONFIG['N_COMPONENTS']} componentes tanto do fluxo quanto do ivar...\")\n",
    "    pca = PCA(n_components=CONFIG[\"N_COMPONENTS\"])\n",
    "    Xf = pca.fit_transform(Xf)\n",
    "    Xivar_raw = pca.transform(Xivar_raw)\n",
    "    print(f\"Nova forma dos dados de fluxo e ivar: {Xf.shape}\")\n",
    "    # --- 7. Split & Scale de uma só vez ---\n",
    "    Xf_k_s, Xz_k_s, Xivar_k_s, y_k, Xf_test_s, Xz_test_s, Xivar_test_s, y_test = split_and_scale(\n",
    "        Xf, Xz_raw, Xivar_raw, y_oh, CONFIG\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro durante o split e escalonamento:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ed12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    n_flux_features = Xf_k_s.shape[1]\n",
    "    n_z_features = Xz_k_s.shape[1]\n",
    "    n_ivar_features = Xivar_k_s.shape[1]\n",
    "    n_classes = y_k.shape[1]\n",
    "\n",
    "    model_builder = build_model_tuner_factory(\n",
    "        n_flux_features, n_z_features, n_ivar_features, n_classes, CONFIG\n",
    "    )\n",
    "    \n",
    "    tuner = kt.RandomSearch(\n",
    "        model_builder,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=10,\n",
    "        executions_per_trial=1,\n",
    "        directory='ktuner_dir',\n",
    "        project_name='tune_layers_units'\n",
    ")\n",
    "\n",
    "    tuner.search([Xf_k_s, Xz_k_s, Xivar_k_s], y_k, validation_split=0.1, epochs=20, batch_size=128)\n",
    "\n",
    "    best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "    print('Best number of layers:', best_hp.get('num_layers'))\n",
    "\n",
    "    best_model = tuner.hypermodel.build(best_hp)\n",
    "    # --- 8. Validação Cruzada ---\n",
    "    print(f\"→ {len(Xf_k_s)} espectros para K-fold e {len(Xf_test_s)} para teste final.\")\n",
    "    metrics, histories = cross_validate(Xf_k_s, Xz_k_s, Xivar_k_s, y_k, CONFIG, best_hp)\n",
    "    losses, accs = zip(*metrics)\n",
    "    print(f\"\\nCV → Acurácia média: {np.mean(accs):.4f} ± {np.std(accs):.4f} | Perda média: {np.mean(losses):.4f}\")\n",
    "\n",
    "    # --- 9. Plota históricos por fold ---\n",
    "    plot_histories(histories)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Erro durante a validação cruzada:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0732c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what the best hyperparameters are\n",
    "print(\"Melhores hiperparâmetros encontrados:\")\n",
    "print(best_hp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e474e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # --- 10. Treinamento final e avaliação no conjunto de teste ---\n",
    "    model = train_final_and_evaluate(\n",
    "        Xf_k_s, Xz_k_s, Xivar_k_s, y_k,\n",
    "        Xf_test_s, Xz_test_s, Xivar_test_s, y_test,\n",
    "        le, CONFIG, best_hp\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Erro durante a execução otimizada:\", e)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc21ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#png do modelo\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/spectra_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfe393",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del best_model \n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca41dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('saved_model/spectra_model.h5')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Evaluate the loaded model\n",
    "# medir quanto tempo leva para avaliar o modelo\n",
    "import time\n",
    "start_time = time.time()\n",
    "loss, acc = loaded_model.evaluate([Xf_test_s, Xz_test_s, Xivar_test_s], y_test, verbose=0)\n",
    "end_time = time.time()\n",
    "print(f\"Tempo de avaliação: {end_time - start_time:.2f} segundos\")\n",
    "print(f\"Acurácia do modelo carregado: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
